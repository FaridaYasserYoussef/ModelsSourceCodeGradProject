{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a237bb",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e034071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv1D, Flatten, concatenate, Input\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.preprocessing\n",
    "\n",
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "#Splitting data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Building Transformer\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e46a7",
   "metadata": {},
   "source": [
    "##### 1) Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd2538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files_train = []\n",
    "for i in os.listdir(\"zigzag/train\"):\n",
    "    files_train.append([\"zigzag/train/\"+i, \"zigzag\"])\n",
    "    \n",
    "    \n",
    "for i in os.listdir(\"speedbumppassing/train\"):\n",
    "    files_train.append([\"speedbumppassing/train/\"+i, \"speedbump\"])\n",
    "    \n",
    "for i in os.listdir(\"potholes/train\"):\n",
    "    files_train.append([\"potholes/train/\"+i, \"pothole\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08271598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files_test = []\n",
    "for i in os.listdir(\"zigzag/test\"):\n",
    "    files_test.append([\"zigzag/test/\"+i, \"zigzag\"])\n",
    "    \n",
    "    \n",
    "for i in os.listdir(\"speedbumppassing/test\"):\n",
    "    files_test.append([\"speedbumppassing/test/\"+i, \"speedbump\"])\n",
    "    \n",
    "for i in os.listdir(\"potholes/test\"):\n",
    "    files_test.append([\"potholes/test/\"+i, \"pothole\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6333138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def csv2numpy(file_name):\n",
    "    \"\"\"Read multidimensional signal from file\"\"\"\n",
    "    # Read data from file.\n",
    "    data = np.genfromtxt(file_name, delimiter=\",\", skip_header=1)\n",
    "    # Return all columns but the first one (as it is the index).\n",
    "    return data[:, 1:-1]\n",
    "\n",
    "\n",
    "\n",
    "def get_crops(files, length, discard_start, discard_end, padding_mode=None):\n",
    "        \"\"\"Return list with crops from files.\"\"\"\n",
    "        crops = []\n",
    "        # Iterate over data files.\n",
    "        for file, class_ in files:\n",
    "            # Read from file.\n",
    "            signal = csv2numpy(file)\n",
    "            # Crop start and end.\n",
    "            signal = signal[discard_start:(signal.shape[0] - discard_end)]\n",
    "            windows, remainder = divmod(signal.shape[0], length)\n",
    "            if padding_mode and remainder != 0:\n",
    "                # Apply padding with given padding mode.\n",
    "                padding = length * (windows + 1) - signal.shape[0]\n",
    "                signal = np.pad(signal, ((0, padding), (0, 0)), padding_mode)\n",
    "            elif padding_mode is None:\n",
    "                # Crop the end.\n",
    "                signal = signal[:(length * windows)]\n",
    "            # Obtain crops from <discard_start> to <discard-end>.\n",
    "            for i in range(0, signal.shape[0], length):\n",
    "                crop = signal[i:(i + length)]\n",
    "                crops.append([crop, class_])\n",
    "\n",
    "        return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d63b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_train = get_crops(files_train, 245, 50, 50)\n",
    "crops_test = get_crops(files_test, 245, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6fbf02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "label_encoder.fit([\"zigzag\", \"speedbump\", \"pothole\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22236a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in crops_train:\n",
    "    X_train.append(i[0])\n",
    "    y_train.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f027eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for i in crops_test:\n",
    "    X_test.append(i[0])\n",
    "    y_test.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da6bf501",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e3069ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(label_encoder.transform(y_train))\n",
    "y_test = to_categorical(label_encoder.transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15f85870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16618, 245, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b17790",
   "metadata": {},
   "source": [
    "##### 3) Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47574519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7203c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52744c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\Suzan Hatem\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=2,\n",
    "    num_heads=4,\n",
    "    ff_dim=3,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.3,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=[\"categorical_accuracy\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "543e1668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 245, 6)]             0         []                            \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 245, 6)               222       ['input_1[0][0]',             \n",
      " iHeadAttention)                                                     'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 245, 6)               0         ['multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 245, 6)               12        ['dropout[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 245, 6)               0         ['layer_normalization[0][0]', \n",
      " Lambda)                                                             'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 245, 3)               21        ['tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 245, 3)               0         ['conv1d[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 245, 6)               24        ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 245, 6)               12        ['conv1d_1[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 245, 6)               0         ['layer_normalization_1[0][0]'\n",
      " OpLambda)                                                          , 'tf.__operators__.add[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 245, 6)               222       ['tf.__operators__.add_1[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'tf.__operators__.add_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 245, 6)               0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 245, 6)               12        ['dropout_2[0][0]']           \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 245, 6)               0         ['layer_normalization_2[0][0]'\n",
      " OpLambda)                                                          , 'tf.__operators__.add_1[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 245, 3)               21        ['tf.__operators__.add_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 245, 3)               0         ['conv1d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 245, 6)               24        ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 245, 6)               12        ['conv1d_3[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TF  (None, 245, 6)               0         ['layer_normalization_3[0][0]'\n",
      " OpLambda)                                                          , 'tf.__operators__.add_2[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 245, 6)               222       ['tf.__operators__.add_3[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'tf.__operators__.add_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 245, 6)               0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 245, 6)               12        ['dropout_4[0][0]']           \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TF  (None, 245, 6)               0         ['layer_normalization_4[0][0]'\n",
      " OpLambda)                                                          , 'tf.__operators__.add_3[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 245, 3)               21        ['tf.__operators__.add_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 245, 3)               0         ['conv1d_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 245, 6)               24        ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 245, 6)               12        ['conv1d_5[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tf.__operators__.add_5 (TF  (None, 245, 6)               0         ['layer_normalization_5[0][0]'\n",
      " OpLambda)                                                          , 'tf.__operators__.add_4[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 245, 6)               222       ['tf.__operators__.add_5[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'tf.__operators__.add_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 245, 6)               0         ['multi_head_attention_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 245, 6)               12        ['dropout_6[0][0]']           \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TF  (None, 245, 6)               0         ['layer_normalization_6[0][0]'\n",
      " OpLambda)                                                          , 'tf.__operators__.add_5[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 245, 3)               21        ['tf.__operators__.add_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 245, 3)               0         ['conv1d_6[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 245, 6)               24        ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 245, 6)               12        ['conv1d_7[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TF  (None, 245, 6)               0         ['layer_normalization_7[0][0]'\n",
      " OpLambda)                                                          , 'tf.__operators__.add_6[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 245)                  0         ['tf.__operators__.add_7[0][0]\n",
      " GlobalAveragePooling1D)                                            ']                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 128)                  31488     ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 128)                  0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 3)                    387       ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 33039 (129.06 KB)\n",
      "Trainable params: 33039 (129.06 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2310235",
   "metadata": {},
   "source": [
    "##### 4) Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ce202a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From D:\\Users\\Suzan Hatem\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\Suzan Hatem\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "260/260 [==============================] - 330s 1s/step - loss: 0.5542 - categorical_accuracy: 0.8248 - val_loss: 1.0627 - val_categorical_accuracy: 0.6891\n",
      "Epoch 2/50\n",
      "260/260 [==============================] - 297s 1s/step - loss: 0.3760 - categorical_accuracy: 0.9148 - val_loss: 1.0228 - val_categorical_accuracy: 0.6891\n",
      "Epoch 3/50\n",
      "260/260 [==============================] - 298s 1s/step - loss: 0.3387 - categorical_accuracy: 0.9168 - val_loss: 1.1098 - val_categorical_accuracy: 0.6916\n",
      "Epoch 4/50\n",
      "260/260 [==============================] - 347s 1s/step - loss: 0.3215 - categorical_accuracy: 0.9180 - val_loss: 1.0847 - val_categorical_accuracy: 0.6940\n",
      "Epoch 5/50\n",
      "260/260 [==============================] - 290s 1s/step - loss: 0.3074 - categorical_accuracy: 0.9183 - val_loss: 1.1012 - val_categorical_accuracy: 0.6952\n",
      "Epoch 6/50\n",
      "260/260 [==============================] - 315s 1s/step - loss: 0.2961 - categorical_accuracy: 0.9197 - val_loss: 1.0572 - val_categorical_accuracy: 0.6940\n",
      "Epoch 7/50\n",
      "260/260 [==============================] - 282s 1s/step - loss: 0.2850 - categorical_accuracy: 0.9202 - val_loss: 1.1394 - val_categorical_accuracy: 0.6940\n",
      "Epoch 8/50\n",
      "260/260 [==============================] - 291s 1s/step - loss: 0.2754 - categorical_accuracy: 0.9204 - val_loss: 1.1613 - val_categorical_accuracy: 0.6940\n",
      "Epoch 9/50\n",
      "260/260 [==============================] - 287s 1s/step - loss: 0.2689 - categorical_accuracy: 0.9206 - val_loss: 1.0995 - val_categorical_accuracy: 0.6977\n",
      "Epoch 10/50\n",
      "260/260 [==============================] - 291s 1s/step - loss: 0.2626 - categorical_accuracy: 0.9215 - val_loss: 1.1346 - val_categorical_accuracy: 0.7001\n",
      "Epoch 11/50\n",
      "260/260 [==============================] - 285s 1s/step - loss: 0.2547 - categorical_accuracy: 0.9228 - val_loss: 1.0473 - val_categorical_accuracy: 0.7099\n",
      "Epoch 12/50\n",
      "260/260 [==============================] - 292s 1s/step - loss: 0.2474 - categorical_accuracy: 0.9232 - val_loss: 1.0238 - val_categorical_accuracy: 0.7099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x20d329fbd60>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "history=model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data = (X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1bee4",
   "metadata": {},
   "source": [
    "##### 5) Finding Training and Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "718c9093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and testing accuracy\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d1210a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 5s 174ms/step - loss: 1.0228 - categorical_accuracy: 0.6891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0227810144424438, 0.6891064643859863)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss,test_acc=model.evaluate(X_test, y_test)\n",
    "test_loss,test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f30236",
   "metadata": {},
   "source": [
    "##### 6) Compute and plot the confusion matrix for the three classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f51157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 5s 213ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00       127\n",
      "     neutral       0.69      1.00      0.82       564\n",
      "    positive       0.00      0.00      0.00       126\n",
      "\n",
      "    accuracy                           0.69       817\n",
      "   macro avg       0.23      0.33      0.27       817\n",
      "weighted avg       0.48      0.69      0.56       817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Suzan Hatem\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Users\\Suzan Hatem\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Users\\Suzan Hatem\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict(X_test)\n",
    "class_labels = ['negative', 'positive','neutral']\n",
    "predicted_class_labels = [class_labels[np.argmax(pred)] for pred in predictions]\n",
    "actual_class_labels=[class_labels[np.argmax(actual)] for actual in y_test]\n",
    "print(classification_report(actual_class_labels, predicted_class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad412a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(actual_class_labels, predicted_class_labels, labels=class_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Purples\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6262b47",
   "metadata": {},
   "source": [
    "##### 7) Saving the final results in Excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ff1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_per_class = precision_score(actual_class_labels, predicted_class_labels, average=None)\n",
    "recall_per_class = recall_score(actual_class_labels, predicted_class_labels, average=None)\n",
    "\n",
    "print(\"Precision per class:\", precision_per_class)\n",
    "print(\"Recall per class:\", recall_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e7aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro = f1_score(actual_class_labels, predicted_class_labels, average='micro')\n",
    "f1_macro = f1_score(actual_class_labels, predicted_class_labels, average='macro')\n",
    "\n",
    "print(\"Micro F1 Score:\", f1_micro)\n",
    "print(\"Macro F1 Score:\", f1_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'Transformer_Results':[max(history.history['accuracy']),test_acc,f1_micro,f1_macro,precision_per_class[0],precision_per_class[1],precision_per_class[2],recall_per_class[0],recall_per_class[1],recall_per_class[2]]}\n",
    "result=pd.DataFrame(data,index=['Training Accuracy','Testing Accuracy','Micro F1 Score','Macro F1 Score','Precision for Negative sentiment','Precision for Positive sentiment','Precision for Neutral sentiment','Recall for Negative sentiment','Recall for Positive sentiment','Recall for Neutral sentiment'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f42457",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('Results.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
