{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eeddeb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Module for data-related stuff.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# class DrivingDataset:\n",
    "\n",
    "#     CLASSES = {\n",
    "#         \"speedbumppassing\": \"speedbumppassing\",\n",
    "#         \"zigzag\": \"zigzag\",\n",
    "#     }\n",
    "\n",
    "#     FEATURES = [\n",
    "#         \"ax\",\n",
    "#         \"ay\",\n",
    "#         \"az\",\n",
    "#         \"wx\",\n",
    "#         \"wy\",\n",
    "#         \"wz\"\n",
    "#     ]\n",
    "\n",
    "#     def __init__(self, data_root, unwrapped_attitude=False,\n",
    "#                  metadata_file=None):\n",
    "#         self.data_root = pathlib.Path(data_root)\n",
    "#         self.files = []\n",
    "#         self.unwrapped_attitude = unwrapped_attitude\n",
    "#         self.metadata = {}  # Dictionary with participant codes as keys.\n",
    "\n",
    "#         # Save each CSV file and infer class from filename.\n",
    "#         for csv_ in self.data_root.glob(\"**/*.csv\"):\n",
    "#             class_ = str(csv_.parent.stem)[:3]\n",
    "#             if class_ in self.CLASSES.keys():\n",
    "#                 self.files.append([csv_, class_])\n",
    "\n",
    "#         # Read metadata form given file.\n",
    "#         if metadata_file:\n",
    "#             with open(metadata_file, newline=\"\") as metadata:\n",
    "#                 csv_reader = csv.reader(metadata)\n",
    "#                 next(csv_reader, None)  # skip the headers\n",
    "#                 for row in csv_reader:\n",
    "#                     self.metadata[row[0]] = list(map(int, row[1:]))\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         file_, class_ = self.files[item]\n",
    "#         signals = csv2numpy(file_)\n",
    "\n",
    "#         if self.unwrapped_attitude:\n",
    "#             # Unwrap attitude signals.\n",
    "#             for i in range(3):\n",
    "#                 signals[:, i] = np.unwrap(signals[:, i])\n",
    "\n",
    "#         if self.metadata:\n",
    "#             # Read metadata and return as extra element.\n",
    "#             metadata = self.metadata[file_.stem.split(\"_\")[1]]\n",
    "#             return signals, class_, metadata\n",
    "#         return signals, class_\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.files)\n",
    "\n",
    "\n",
    "# class HARDatasetCrops(HARDataset):\n",
    "#     \"\"\"Dataset with fixed-length crops.\n",
    "\n",
    "#     Args:\n",
    "#         data_root -- string. Path to data directory.\n",
    "#         length -- int. Crops length.\n",
    "#         discard_start -- int. Number of samples to discard from start.\n",
    "#         discard_end -- int. Number of samples to discard from end.\n",
    "#         unwrapped_attitude -- bool. Whether to unwrap attitude signals.\n",
    "#         padding_mode -- None or string. If None, the samples not fitting in\n",
    "#                 integer number of windows will be discarded. If string,\n",
    "#                 the value will be passed to numpy's pad function.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, data_root, length, discard_start, discard_end,\n",
    "#                  unwrapped_attitude=True, padding_mode=None,\n",
    "#                  metadata_file=None):\n",
    "#         super().__init__(data_root, unwrapped_attitude=unwrapped_attitude,\n",
    "#                          metadata_file=metadata_file)\n",
    "#         self.length = length\n",
    "#         self.discard_start = discard_start\n",
    "#         self.discard_end = discard_end\n",
    "#         self.padding_mode = padding_mode\n",
    "\n",
    "#         self.crops = self.get_crops()\n",
    "\n",
    "#     def get_crops(self):\n",
    "#         \"\"\"Return list with crops from files.\"\"\"\n",
    "#         crops = []\n",
    "#         # Iterate over data files.\n",
    "#         for file, class_ in self.files:\n",
    "#             # Read from file.\n",
    "#             signal = csv2numpy(file)\n",
    "#             # Crop start and end.\n",
    "#             signal = signal[self.discard_start:(signal.shape[0] - self.discard_end)]\n",
    "#             windows, remainder = divmod(signal.shape[0], self.length)\n",
    "#             if self.padding_mode and remainder != 0:\n",
    "#                 # Apply padding with given padding mode.\n",
    "#                 padding = self.length * (windows + 1) - signal.shape[0]\n",
    "#                 signal = np.pad(signal, ((0, padding), (0, 0)), self.padding_mode)\n",
    "#             elif self.padding_mode is None:\n",
    "#                 # Crop the end.\n",
    "#                 signal = signal[:(self.length * windows)]\n",
    "#             # Obtain crops from <discard_start> to <discard-end>.\n",
    "#             for i in range(0, signal.shape[0], self.length):\n",
    "#                 crop = signal[i:(i + self.length)]\n",
    "#                 if self.unwrapped_attitude:\n",
    "#                     # Unwrap phase of first 3 features (attitude signals).\n",
    "#                     for s in range(3):\n",
    "#                         crop[:, s] = np.unwrap(crop[:, s])\n",
    "#                 if self.metadata:\n",
    "#                     # Read metadata and return as extra element.\n",
    "#                     metadata = self.metadata[file.stem.split(\"_\")[1]]\n",
    "#                     crops.append([crop, class_, metadata])\n",
    "#                 else:\n",
    "#                     crops.append([crop, class_])\n",
    "\n",
    "#         return crops\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         return self.crops[item]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.crops)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     dataset = HARDatasetCrops('motionsense-dataset', 256, 10, 10, True)\n",
    "#     for item in iter(dataset):\n",
    "#         assert item[0].shape == (256, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c19e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv1D, Flatten, concatenate, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f38f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "files_train = []\n",
    "for i in os.listdir(\"zigzag/train\"):\n",
    "    files_train.append([\"zigzag/train/\"+i, \"zigzag\"])\n",
    "print(len(files_train))    \n",
    "    \n",
    "for i in os.listdir(\"speedbumppassing/train\"):\n",
    "    files_train.append([\"speedbumppassing/train/\"+i, \"speedbump\"])\n",
    "    \n",
    "for i in os.listdir(\"potholes/train\"):\n",
    "    files_train.append([\"potholes/train/\"+i, \"pothole\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f72ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "files_test = []\n",
    "for i in os.listdir(\"zigzag/test\"):\n",
    "    files_test.append([\"zigzag/test/\"+i, \"zigzag\"])\n",
    "print(len(files_test))    \n",
    "    \n",
    "for i in os.listdir(\"speedbumppassing/test\"):\n",
    "    files_test.append([\"speedbumppassing/test/\"+i, \"speedbump\"])\n",
    "    \n",
    "for i in os.listdir(\"potholes/test\"):\n",
    "    files_test.append([\"potholes/test/\"+i, \"pothole\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20835a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28211323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def csv2numpy(file_name):\n",
    "    \"\"\"Read multidimensional signal from file\"\"\"\n",
    "    # Read data from file.\n",
    "    data = np.genfromtxt(file_name, delimiter=\",\", skip_header=1)\n",
    "    # Return all columns but the first one (as it is the index).\n",
    "    return data[:, 1:-1]\n",
    "\n",
    "\n",
    "\n",
    "def get_crops(files, length, discard_start, discard_end, padding_mode=None):\n",
    "        \"\"\"Return list with crops from files.\"\"\"\n",
    "        crops = []\n",
    "        # Iterate over data files.\n",
    "        for file, class_ in files:\n",
    "            # Read from file.\n",
    "            signal = csv2numpy(file)\n",
    "            # Crop start and end.\n",
    "            signal = signal[discard_start:(signal.shape[0] - discard_end)]\n",
    "            windows, remainder = divmod(signal.shape[0], length)\n",
    "            if padding_mode and remainder != 0:\n",
    "                # Apply padding with given padding mode.\n",
    "                padding = length * (windows + 1) - signal.shape[0]\n",
    "                signal = np.pad(signal, ((0, padding), (0, 0)), padding_mode)\n",
    "            elif padding_mode is None:\n",
    "                # Crop the end.\n",
    "                signal = signal[:(length * windows)]\n",
    "            # Obtain crops from <discard_start> to <discard-end>.\n",
    "            for i in range(0, signal.shape[0], length):\n",
    "                crop = signal[i:(i + length)]\n",
    "                crops.append([crop, class_])\n",
    "\n",
    "        return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60604054",
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_train = get_crops(files_train, 245, 50, 50)\n",
    "crops_test = get_crops(files_test, 245, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c57c105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "label_encoder.fit([\"zigzag\", \"speedbump\", \"pothole\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d009d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in crops_train:\n",
    "    X_train.append(i[0])\n",
    "    y_train.append(i[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7e966e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zigzag count: 15253\n",
      "speedbump count: 685\n",
      "pothole count: 680\n"
     ]
    }
   ],
   "source": [
    "zigzag_count = y_train.count(\"zigzag\")\n",
    "speedbump_count = y_train.count(\"speedbump\")\n",
    "pothole_count = y_train.count(\"pothole\")\n",
    "\n",
    "print(\"zigzag count:\", zigzag_count)\n",
    "print(\"speedbump count:\", speedbump_count)\n",
    "print(\"pothole count:\", pothole_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c77586b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping: {'pothole': 0, 'speedbump': 1, 'zigzag': 2}\n"
     ]
    }
   ],
   "source": [
    "mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "print(\"Mapping:\", mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28c1f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for i in crops_test:\n",
    "    X_test.append(i[0])\n",
    "    y_test.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc4c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b6bc3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(label_encoder.transform(y_train))\n",
    "y_test = to_categorical(label_encoder.transform(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f332a360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16618, 245, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40c7602f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(817, 245, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e17ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pothole = np.array([1., 0., 0.])\n",
    "speedbump = np.array([0., 1., 0.])\n",
    "zigzag = np.array([0., 0., 1.])\n",
    "\n",
    "zigzagc = np.sum(np.all(y_train == zigzag, axis=1))\n",
    "speedbumpc = np.sum(np.all(y_train == speedbump, axis=1))\n",
    "potholec = np.sum(np.all(y_train == pothole, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "215180ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15253, 685, 680)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zigzagc, speedbumpc, potholec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cbd4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pothole = np.array([1., 0., 0.])\n",
    "speedbump = np.array([0., 1., 0.])\n",
    "zigzag = np.array([0., 0., 1.])\n",
    "\n",
    "zigzagc = np.sum(np.all(y_test == zigzag, axis=1))\n",
    "speedbumpc = np.sum(np.all(y_test == speedbump, axis=1))\n",
    "potholec = np.sum(np.all(y_test == pothole, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba42ed18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(564, 126, 127)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zigzagc, speedbumpc, potholec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "730ad07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16618, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62d02102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(817, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "156a2ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value 0.0: Count 1634\n",
      "Value 1.0: Count 817\n"
     ]
    }
   ],
   "source": [
    "unique_values, counts = np.unique(y_test, return_counts=True)\n",
    "\n",
    "# Display the results\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"Value {value}: Count {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78ba58",
   "metadata": {},
   "source": [
    "# Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7e52dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Sequential()\n",
    "#add model layers\n",
    "clf.add(Conv1D(16, kernel_size=5, activation=\"relu\", input_shape=(245, 6)))\n",
    "clf.add(Conv1D(32, kernel_size=5, activation=\"relu\"))\n",
    "\n",
    "clf.add(Flatten())\n",
    "clf.add(Dense(16, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "clf.add(Dense(3, activation=\"softmax\", kernel_regularizer=keras.regularizers.l2(0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "91740318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 3s 26ms/step - loss: 0.7032 - accuracy: 0.8589 - val_loss: 3.7193 - val_accuracy: 0.3158\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3623 - accuracy: 0.9501 - val_loss: 4.7844 - val_accuracy: 0.2803\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2711 - accuracy: 0.9616 - val_loss: 3.8628 - val_accuracy: 0.3721\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2178 - accuracy: 0.9721 - val_loss: 5.3319 - val_accuracy: 0.3341\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2183 - accuracy: 0.9651 - val_loss: 4.8376 - val_accuracy: 0.3647\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1613 - accuracy: 0.9816 - val_loss: 4.2661 - val_accuracy: 0.3929\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1281 - accuracy: 0.9910 - val_loss: 4.2035 - val_accuracy: 0.4186\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1039 - accuracy: 0.9950 - val_loss: 5.0107 - val_accuracy: 0.3354\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1610 - accuracy: 0.9796 - val_loss: 3.9519 - val_accuracy: 0.4064\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1173 - accuracy: 0.9955 - val_loss: 5.1685 - val_accuracy: 0.3550\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0929 - accuracy: 0.9975 - val_loss: 4.6176 - val_accuracy: 0.3843\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0807 - accuracy: 0.9980 - val_loss: 4.3388 - val_accuracy: 0.3819\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0733 - accuracy: 0.9980 - val_loss: 4.6952 - val_accuracy: 0.3770\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0775 - accuracy: 0.9955 - val_loss: 3.9111 - val_accuracy: 0.4480\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0781 - accuracy: 0.9955 - val_loss: 4.9997 - val_accuracy: 0.3745\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0734 - accuracy: 0.9970 - val_loss: 4.2257 - val_accuracy: 0.4284\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0655 - accuracy: 0.9990 - val_loss: 4.5753 - val_accuracy: 0.3978\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0818 - accuracy: 0.9905 - val_loss: 7.0935 - val_accuracy: 0.3305\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1134 - accuracy: 0.9955 - val_loss: 5.3405 - val_accuracy: 0.4186\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0742 - accuracy: 0.9995 - val_loss: 4.5895 - val_accuracy: 0.4235\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0602 - accuracy: 0.9995 - val_loss: 4.6092 - val_accuracy: 0.3611\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0592 - accuracy: 0.9980 - val_loss: 4.6682 - val_accuracy: 0.4088\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0713 - accuracy: 0.9955 - val_loss: 4.7178 - val_accuracy: 0.3733\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0567 - accuracy: 0.9995 - val_loss: 5.0080 - val_accuracy: 0.3366\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0606 - accuracy: 0.9960 - val_loss: 4.3635 - val_accuracy: 0.3929\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0522 - accuracy: 0.9995 - val_loss: 4.6653 - val_accuracy: 0.3892\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0475 - accuracy: 0.9995 - val_loss: 4.3456 - val_accuracy: 0.4223\n",
      "Epoch 28/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1198 - accuracy: 0.9791 - val_loss: 7.7609 - val_accuracy: 0.3256\n",
      "Epoch 29/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1500 - accuracy: 0.9910 - val_loss: 4.7483 - val_accuracy: 0.4198\n",
      "Epoch 30/30\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0802 - accuracy: 0.9995 - val_loss: 4.3059 - val_accuracy: 0.4015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f824904f50>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "clf.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f05d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a27d6d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 3ms/step - loss: 1.7504 - accuracy: 0.8690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7504258155822754, 0.8690330386161804]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5f1d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = Sequential()\n",
    "#add model layers\n",
    "clf2.add(Conv1D(16, kernel_size=5, activation=\"relu\", input_shape=(245, 6)))\n",
    "clf2.add(Conv1D(32, kernel_size=5, activation=\"relu\"))\n",
    "\n",
    "clf2.add(Flatten())\n",
    "clf2.add(Dense(8, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "clf2.add(Dense(3, activation=\"softmax\", kernel_regularizer=keras.regularizers.l2(0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1680dfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "520/520 [==============================] - 4s 6ms/step - loss: 0.3058 - accuracy: 0.9295 - val_loss: 0.9730 - val_accuracy: 0.8348\n",
      "Epoch 2/30\n",
      "520/520 [==============================] - 3s 6ms/step - loss: 0.1436 - accuracy: 0.9593 - val_loss: 1.1086 - val_accuracy: 0.8507\n",
      "Epoch 3/30\n",
      "520/520 [==============================] - 3s 6ms/step - loss: 0.1062 - accuracy: 0.9792 - val_loss: 1.2036 - val_accuracy: 0.8421\n",
      "Epoch 4/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0827 - accuracy: 0.9844 - val_loss: 1.3781 - val_accuracy: 0.8311\n",
      "Epoch 5/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0686 - accuracy: 0.9883 - val_loss: 1.6799 - val_accuracy: 0.8482\n",
      "Epoch 6/30\n",
      "520/520 [==============================] - 3s 6ms/step - loss: 0.0553 - accuracy: 0.9917 - val_loss: 1.3522 - val_accuracy: 0.8690\n",
      "Epoch 7/30\n",
      "520/520 [==============================] - 3s 6ms/step - loss: 0.0645 - accuracy: 0.9895 - val_loss: 1.4544 - val_accuracy: 0.8568\n",
      "Epoch 8/30\n",
      "520/520 [==============================] - 3s 6ms/step - loss: 0.0528 - accuracy: 0.9933 - val_loss: 1.0653 - val_accuracy: 0.8727\n",
      "Epoch 9/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0501 - accuracy: 0.9928 - val_loss: 1.0506 - val_accuracy: 0.8494\n",
      "Epoch 10/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0435 - accuracy: 0.9932 - val_loss: 1.3521 - val_accuracy: 0.8617\n",
      "Epoch 11/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0516 - accuracy: 0.9930 - val_loss: 0.9219 - val_accuracy: 0.8164\n",
      "Epoch 12/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0390 - accuracy: 0.9948 - val_loss: 1.2351 - val_accuracy: 0.8739\n",
      "Epoch 13/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0395 - accuracy: 0.9945 - val_loss: 1.1894 - val_accuracy: 0.8690\n",
      "Epoch 14/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0441 - accuracy: 0.9940 - val_loss: 1.1218 - val_accuracy: 0.8727\n",
      "Epoch 15/30\n",
      "520/520 [==============================] - 3s 6ms/step - loss: 0.0394 - accuracy: 0.9943 - val_loss: 1.5137 - val_accuracy: 0.8580\n",
      "Epoch 16/30\n",
      "520/520 [==============================] - 3s 6ms/step - loss: 0.0383 - accuracy: 0.9949 - val_loss: 1.5988 - val_accuracy: 0.8666\n",
      "Epoch 17/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0428 - accuracy: 0.9948 - val_loss: 1.0998 - val_accuracy: 0.8641\n",
      "Epoch 18/30\n",
      "520/520 [==============================] - 3s 6ms/step - loss: 0.0320 - accuracy: 0.9959 - val_loss: 1.2333 - val_accuracy: 0.8641\n",
      "Epoch 19/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0311 - accuracy: 0.9963 - val_loss: 1.1700 - val_accuracy: 0.8641\n",
      "Epoch 20/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0403 - accuracy: 0.9945 - val_loss: 1.3988 - val_accuracy: 0.8654\n",
      "Epoch 21/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0304 - accuracy: 0.9961 - val_loss: 1.1815 - val_accuracy: 0.8703\n",
      "Epoch 22/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0318 - accuracy: 0.9960 - val_loss: 1.4274 - val_accuracy: 0.8666\n",
      "Epoch 23/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0286 - accuracy: 0.9971 - val_loss: 1.0941 - val_accuracy: 0.8641\n",
      "Epoch 24/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0395 - accuracy: 0.9945 - val_loss: 1.3412 - val_accuracy: 0.8507\n",
      "Epoch 25/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0291 - accuracy: 0.9966 - val_loss: 1.8113 - val_accuracy: 0.8641\n",
      "Epoch 26/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0335 - accuracy: 0.9959 - val_loss: 1.1178 - val_accuracy: 0.8470\n",
      "Epoch 27/30\n",
      "520/520 [==============================] - 3s 6ms/step - loss: 0.0307 - accuracy: 0.9972 - val_loss: 1.4770 - val_accuracy: 0.8629\n",
      "Epoch 28/30\n",
      "520/520 [==============================] - 3s 6ms/step - loss: 0.0324 - accuracy: 0.9960 - val_loss: 1.0999 - val_accuracy: 0.8703\n",
      "Epoch 29/30\n",
      "520/520 [==============================] - 3s 5ms/step - loss: 0.0299 - accuracy: 0.9964 - val_loss: 0.9932 - val_accuracy: 0.8703\n",
      "Epoch 30/30\n",
      "520/520 [==============================] - 3s 6ms/step - loss: 0.0266 - accuracy: 0.9974 - val_loss: 1.0705 - val_accuracy: 0.8715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x240a43a8510>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "clf2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c540ceb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 2ms/step - loss: 1.0705 - accuracy: 0.8715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0704624652862549, 0.8714810013771057]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba05b00",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b11cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "#Splitting data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Building Transformer\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40455572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3019cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72514aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 245, 6)]             0         []                            \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 245, 6)               222       ['input_1[0][0]',             \n",
      " iHeadAttention)                                                     'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 245, 6)               0         ['multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 245, 6)               12        ['dropout[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 245, 6)               0         ['layer_normalization[0][0]', \n",
      " Lambda)                                                             'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 245, 3)               21        ['tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 245, 3)               0         ['conv1d[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 245, 6)               24        ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 245, 6)               12        ['conv1d_1[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 245, 6)               0         ['layer_normalization_1[0][0]'\n",
      " OpLambda)                                                          , 'tf.__operators__.add[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 245, 6)               222       ['tf.__operators__.add_1[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'tf.__operators__.add_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 245, 6)               0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 245, 6)               12        ['dropout_2[0][0]']           \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 245, 6)               0         ['layer_normalization_2[0][0]'\n",
      " OpLambda)                                                          , 'tf.__operators__.add_1[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 245, 3)               21        ['tf.__operators__.add_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 245, 3)               0         ['conv1d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 245, 6)               24        ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 245, 6)               12        ['conv1d_3[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TF  (None, 245, 6)               0         ['layer_normalization_3[0][0]'\n",
      " OpLambda)                                                          , 'tf.__operators__.add_2[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 245, 6)               222       ['tf.__operators__.add_3[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'tf.__operators__.add_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 245, 6)               0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 245, 6)               12        ['dropout_4[0][0]']           \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TF  (None, 245, 6)               0         ['layer_normalization_4[0][0]'\n",
      " OpLambda)                                                          , 'tf.__operators__.add_3[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 245, 3)               21        ['tf.__operators__.add_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 245, 3)               0         ['conv1d_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 245, 6)               24        ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 245, 6)               12        ['conv1d_5[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tf.__operators__.add_5 (TF  (None, 245, 6)               0         ['layer_normalization_5[0][0]'\n",
      " OpLambda)                                                          , 'tf.__operators__.add_4[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 245, 6)               222       ['tf.__operators__.add_5[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'tf.__operators__.add_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 245, 6)               0         ['multi_head_attention_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 245, 6)               12        ['dropout_6[0][0]']           \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TF  (None, 245, 6)               0         ['layer_normalization_6[0][0]'\n",
      " OpLambda)                                                          , 'tf.__operators__.add_5[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 245, 3)               21        ['tf.__operators__.add_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 245, 3)               0         ['conv1d_6[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 245, 6)               24        ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 245, 6)               12        ['conv1d_7[0][0]']            \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TF  (None, 245, 6)               0         ['layer_normalization_7[0][0]'\n",
      " OpLambda)                                                          , 'tf.__operators__.add_6[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 245)                  0         ['tf.__operators__.add_7[0][0]\n",
      " GlobalAveragePooling1D)                                            ']                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 128)                  31488     ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 128)                  0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 3)                    387       ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 33039 (129.06 KB)\n",
      "Trainable params: 33039 (129.06 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "260/260 [==============================] - 474s 2s/step - loss: 0.5366 - categorical_accuracy: 0.8484 - val_loss: 1.0736 - val_categorical_accuracy: 0.6891\n",
      "Epoch 2/50\n",
      "260/260 [==============================] - 458s 2s/step - loss: 0.3846 - categorical_accuracy: 0.9144 - val_loss: 1.0221 - val_categorical_accuracy: 0.6903\n",
      "Epoch 3/50\n",
      "260/260 [==============================] - 453s 2s/step - loss: 0.3550 - categorical_accuracy: 0.9171 - val_loss: 1.0278 - val_categorical_accuracy: 0.6903\n",
      "Epoch 4/50\n",
      "260/260 [==============================] - 475s 2s/step - loss: 0.3350 - categorical_accuracy: 0.9181 - val_loss: 1.0018 - val_categorical_accuracy: 0.6916\n",
      "Epoch 5/50\n",
      "260/260 [==============================] - 456s 2s/step - loss: 0.3252 - categorical_accuracy: 0.9186 - val_loss: 1.0331 - val_categorical_accuracy: 0.6952\n",
      "Epoch 6/50\n",
      "260/260 [==============================] - 457s 2s/step - loss: 0.3165 - categorical_accuracy: 0.9191 - val_loss: 1.0538 - val_categorical_accuracy: 0.6952\n",
      "Epoch 7/50\n",
      "260/260 [==============================] - 456s 2s/step - loss: 0.3089 - categorical_accuracy: 0.9204 - val_loss: 1.0231 - val_categorical_accuracy: 0.6952\n",
      "Epoch 8/50\n",
      "260/260 [==============================] - 502s 2s/step - loss: 0.3015 - categorical_accuracy: 0.9209 - val_loss: 1.0006 - val_categorical_accuracy: 0.6965\n",
      "Epoch 9/50\n",
      "260/260 [==============================] - 291s 1s/step - loss: 0.2956 - categorical_accuracy: 0.9207 - val_loss: 1.0088 - val_categorical_accuracy: 0.7026\n",
      "Epoch 10/50\n",
      "260/260 [==============================] - 254s 976ms/step - loss: 0.2915 - categorical_accuracy: 0.9214 - val_loss: 1.0221 - val_categorical_accuracy: 0.7062\n",
      "Epoch 11/50\n",
      "260/260 [==============================] - 269s 1s/step - loss: 0.2837 - categorical_accuracy: 0.9224 - val_loss: 1.0444 - val_categorical_accuracy: 0.7050\n",
      "Epoch 12/50\n",
      "260/260 [==============================] - 249s 957ms/step - loss: 0.2777 - categorical_accuracy: 0.9223 - val_loss: 0.9925 - val_categorical_accuracy: 0.7099\n",
      "Epoch 13/50\n",
      "260/260 [==============================] - 250s 962ms/step - loss: 0.2698 - categorical_accuracy: 0.9232 - val_loss: 0.9984 - val_categorical_accuracy: 0.7246\n",
      "Epoch 14/50\n",
      "260/260 [==============================] - 256s 984ms/step - loss: 0.2645 - categorical_accuracy: 0.9238 - val_loss: 1.0554 - val_categorical_accuracy: 0.7234\n",
      "Epoch 15/50\n",
      "260/260 [==============================] - 251s 966ms/step - loss: 0.2551 - categorical_accuracy: 0.9250 - val_loss: 1.0265 - val_categorical_accuracy: 0.7319\n",
      "Epoch 16/50\n",
      "260/260 [==============================] - 250s 963ms/step - loss: 0.2504 - categorical_accuracy: 0.9257 - val_loss: 1.0760 - val_categorical_accuracy: 0.7307\n",
      "Epoch 17/50\n",
      "260/260 [==============================] - 249s 958ms/step - loss: 0.2418 - categorical_accuracy: 0.9273 - val_loss: 1.1043 - val_categorical_accuracy: 0.7319\n",
      "Epoch 18/50\n",
      "260/260 [==============================] - 265s 1s/step - loss: 0.2392 - categorical_accuracy: 0.9265 - val_loss: 1.0715 - val_categorical_accuracy: 0.7344\n",
      "Epoch 19/50\n",
      "260/260 [==============================] - 251s 965ms/step - loss: 0.2342 - categorical_accuracy: 0.9275 - val_loss: 1.0327 - val_categorical_accuracy: 0.7368\n",
      "Epoch 20/50\n",
      "260/260 [==============================] - 251s 965ms/step - loss: 0.2232 - categorical_accuracy: 0.9289 - val_loss: 1.0761 - val_categorical_accuracy: 0.7381\n",
      "Epoch 21/50\n",
      "260/260 [==============================] - 274s 1s/step - loss: 0.2212 - categorical_accuracy: 0.9292 - val_loss: 1.1101 - val_categorical_accuracy: 0.7356\n",
      "Epoch 22/50\n",
      "260/260 [==============================] - 254s 977ms/step - loss: 0.2172 - categorical_accuracy: 0.9295 - val_loss: 1.1084 - val_categorical_accuracy: 0.7344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f8262cbc90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=2,\n",
    "    num_heads=4,\n",
    "    ff_dim=3,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.3,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data = (X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "201992a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 4s 143ms/step - loss: 0.9925 - categorical_accuracy: 0.7099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9925313591957092, 0.7099143266677856]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c72ba",
   "metadata": {},
   "source": [
    "# CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2886127b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "520/520 [==============================] - 128s 233ms/step - loss: 0.5720 - accuracy: 0.9162 - val_loss: 0.9867 - val_accuracy: 0.6903\n",
      "Epoch 2/30\n",
      "520/520 [==============================] - 121s 232ms/step - loss: 0.3647 - accuracy: 0.9171 - val_loss: 1.2231 - val_accuracy: 0.6891\n",
      "Epoch 3/30\n",
      "520/520 [==============================] - 119s 229ms/step - loss: 0.3543 - accuracy: 0.9171 - val_loss: 0.9693 - val_accuracy: 0.6903\n",
      "Epoch 4/30\n",
      "520/520 [==============================] - 120s 232ms/step - loss: 0.3241 - accuracy: 0.9242 - val_loss: 0.6528 - val_accuracy: 0.8360\n",
      "Epoch 5/30\n",
      "128/520 [======>.......................] - ETA: 1:32 - loss: 0.2821 - accuracy: 0.9363"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    868\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    869\u001b[0m   )\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mflat_call(args)\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    256\u001b[0m     )\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1480\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1481\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1482\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1483\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1484\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1485\u001b[0m   )\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv1D, MaxPooling1D, Flatten, LSTM, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define the model with L2 regularization\n",
    "model = Sequential()\n",
    "\n",
    "# TimeDistributed(Conv1D()) layer with parameters and L2 regularization\n",
    "model.add(TimeDistributed(Conv1D(filters=16, kernel_size=3, activation='relu', kernel_regularizer=l2(0.01)), input_shape=(245, 6, 1)))\n",
    "\n",
    "# TimeDistributed(MaxPooling1D()) layer with parameters and padding\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2, padding='same')))\n",
    "\n",
    "# TimeDistributed(Conv1D()) layer with parameters and L2 regularization\n",
    "model.add(TimeDistributed(Conv1D(filters=8, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.01))))\n",
    "\n",
    "# TimeDistributed(MaxPooling1D()) layer with parameters and padding\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2, padding='same')))\n",
    "\n",
    "# TimeDistributed(Conv1D()) layer with parameters and L2 regularization\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.01))))\n",
    "\n",
    "# TimeDistributed(MaxPooling1D()) layer with parameters and padding\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2, padding='same')))\n",
    "\n",
    "# TimeDistributed(Conv1D()) layer with parameters and L2 regularization\n",
    "model.add(TimeDistributed(Conv1D(filters=8, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.01))))\n",
    "\n",
    "# TimeDistributed(MaxPooling1D()) layer with parameters and padding\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2, padding='same')))\n",
    "\n",
    "# TimeDistributed(Flatten()) layer\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "# Define LSTM model with parameters and L2 regularization\n",
    "model.add(LSTM(units=60, return_sequences=True, kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Additional LSTM layer with L2 regularization\n",
    "model.add(LSTM(units=70, return_sequences=True, kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Flatten layer outside TimeDistributed\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layer with parameters and L2 regularization\n",
    "model.add(Dense(units=3, activation='softmax', kernel_regularizer=l2(0.01)))  # Adjust units to match the number of output classes\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c60ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cebbce9",
   "metadata": {},
   "source": [
    "# TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "278275e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tcn\n",
      "  Downloading keras_tcn-3.5.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras-tcn) (1.24.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras-tcn) (2.14.0)\n",
      "Collecting tensorflow-addons (from keras-tcn)\n",
      "  Obtaining dependency information for tensorflow-addons from https://files.pythonhosted.org/packages/ec/52/047d768c4669db0c059109a88c21a3c71bcda957c46f13967e44b8c7fa4c/tensorflow_addons-0.22.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_addons-0.22.0-cp311-cp311-win_amd64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow-intel==2.14.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow->keras-tcn) (2.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (1.59.2)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow->keras-tcn) (2.14.0)\n",
      "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->keras-tcn)\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow->keras-tcn) (3.2.2)\n",
      "Downloading tensorflow_addons-0.22.0-cp311-cp311-win_amd64.whl (719 kB)\n",
      "   ---------------------------------------- 0.0/719.8 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/719.8 kB 660.6 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 92.2/719.8 kB 1.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 204.8/719.8 kB 1.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 307.2/719.8 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 440.3/719.8 kB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 583.7/719.8 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  716.8/719.8 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 719.8/719.8 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: typeguard, tensorflow-addons, keras-tcn\n",
      "Successfully installed keras-tcn-3.5.0 tensorflow-addons-0.22.0 typeguard-2.13.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: neuralplot 0.0.8 has a non-standard dependency specifier matplotlib>=3.1numpy>=1.16. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of neuralplot or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1880de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TCN Model\n",
    "from tcn import TCN\n",
    "def build_tcn_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Add TCN layer\n",
    "    model.add(TCN(input_shape=input_shape, nb_filters=64, kernel_size=3, dilations=[1, 2, 4, 8], return_sequences=False))\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "input_shape = X_train.shape[1:]\n",
    "model = build_tcn_model(input_shape, 2)\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the Model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438fe0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ee889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
